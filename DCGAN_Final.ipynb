{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DCGAN_Final.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "OiJzfP_d88xN",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "                ############ Imports ############\n",
        "                \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "import time\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AxbIEmw49wuP",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "                ############ Initializations ############            \n",
        "\n",
        "channels = 1\n",
        "height = 64\n",
        "width = 64\n",
        "# MNIST was resized to 64 * 64 for discriminator and generator architecture fitting\n",
        "latent = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WVepUf_2khFX",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "                ############ Importing MNIST data ############                \n",
        "\n",
        "def get_data():\n",
        "    from tensorflow.examples.tutorials.mnist import input_data\n",
        "    mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True, reshape=[])# With reshape gives already 28*28*1 images\n",
        "    return mnist\n",
        "\n",
        "                ############ Normalizing data ############\n",
        "# Scaling in range (-1,1) for generator tanh output\n",
        "def scale(x):\n",
        "    # normalize data\n",
        "    x = (x - 0.5) / 0.5\n",
        "    return x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E7ldhkfXbfjW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Change disc and gen architecture to mirror each other"
      ]
    },
    {
      "metadata": {
        "id": "hX3LbUcp_rDJ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "              ############ Defining Discriminator ############\n",
        "  \n",
        "  def discriminator(x, dropout_rate = 0., is_training = True, reuse = False):\n",
        "    with tf.variable_scope('Discriminator', reuse = reuse):\n",
        "      \n",
        "      print('Discriminator architecture: ')\n",
        "      #Layer 1\n",
        "      conv1 = tf.layers.conv2d(x, 128, kernel_size = [4,4], strides = [2,2],\n",
        "                              padding = 'same', activation = tf.nn.leaky_relu, name = 'conv1')# ?*32*32*128\n",
        "      print(conv1.shape)\n",
        "      #No batch-norm for input layer\n",
        "      dropout1 = tf.nn.dropout(conv1, dropout_rate)\n",
        "      \n",
        "      #Layer2\n",
        "      conv2 = tf.layers.conv2d(dropout1, 256, kernel_size = [4,4], strides = [2,2],\n",
        "                              padding = 'same', activation = tf.nn.leaky_relu, name = 'conv2')# ?*16*16*256\n",
        "      batch2 = tf.layers.batch_normalization(conv2, training = is_training)\n",
        "      dropout2 = tf.nn.dropout(batch2, dropout_rate)\n",
        "      print(conv2.shape)\n",
        "      #Layer3\n",
        "      \"\"\"conv3 = tf.layers.conv2d(dropout2, 512, kernel_size = [4,4], strides = [2,2],\n",
        "                              padding = 'same', activation = tf.nn.leaky_relu, name = 'conv3')\n",
        "      batch3 = tf.layers.batch_normalization(conv3, training = is_training)\n",
        "      dropout3 = tf.nn.dropout(batch3, dropout_rate)\n",
        "      print(conv3.shape)\"\"\"\n",
        "      #Layer4\n",
        "      conv4 = tf.layers.conv2d(dropout2, 1024, kernel_size = [4,4], strides = [4,4],\n",
        "                              padding = 'same', activation = tf.nn.leaky_relu, name = 'conv4')# ?*4*4*1024\n",
        "      batch4 = tf.layers.batch_normalization(conv4, training = is_training)\n",
        "      dropout4 = tf.nn.dropout(batch4, dropout_rate)\n",
        "      print(conv4.shape)\n",
        "      # Layer 5\n",
        "      D_logits = tf.layers.conv2d(dropout4, 1, kernel_size = [4,4], strides = [1,1],\n",
        "                              padding = 'valid', activation = None, name = 'conv5')# ?*1*1*1\n",
        "      output = tf.nn.sigmoid(D_logits)\n",
        "      print(D_logits.shape)\n",
        "      return D_logits,output\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "635RFlWgFk8A",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "              ############ Defining Generator ############\n",
        "                \n",
        "def generator(z, dropout_rate = 0., is_training = True, reuse = False):\n",
        "  with tf.variable_scope('Generator', reuse = reuse):\n",
        "    \n",
        "    print('\\n Generator architecture: ')\n",
        "    #Layer 1\n",
        "    deconv1 = tf.layers.conv2d_transpose(z, 1024, kernel_size = [4,4],\n",
        "                                         strides = [1,1], padding = 'valid',\n",
        "                                        activation = tf.nn.relu, name = 'deconv1')# ?*4*4*1024\n",
        "    batch1 = tf.layers.batch_normalization(deconv1, training = is_training)\n",
        "    dropout1 = tf.nn.dropout(batch1, dropout_rate)\n",
        "    print(deconv1.shape)\n",
        "    #Layer 2\n",
        "    \"\"\"deconv2 = tf.layers.conv2d_transpose(dropout1, 512, kernel_size = [4,4],\n",
        "                                         strides = [2,2], padding = 'same',\n",
        "                                        activation = tf.nn.relu, name = 'deconv2')\n",
        "    batch2 = tf.layers.batch_normalization(deconv2, training = is_training)\n",
        "    dropout2 = tf.nn.dropout(batch2, dropout_rate)\n",
        "    print(deconv2.shape)\"\"\"\n",
        "    #Layer 3\n",
        "    deconv3 = tf.layers.conv2d_transpose(dropout1, 256, kernel_size = [4,4],\n",
        "                                         strides = [4,4], padding = 'same',\n",
        "                                        activation = tf.nn.relu, name = 'deconv3')# ?*16*16*256\n",
        "    batch3 = tf.layers.batch_normalization(deconv3, training = is_training)\n",
        "    dropout3 = tf.nn.dropout(batch3, dropout_rate)\n",
        "    print(deconv3.shape)\n",
        "    #Layer 4\n",
        "    deconv4 = tf.layers.conv2d_transpose(dropout3, 128, kernel_size = [4,4],\n",
        "                                         strides = [2,2], padding = 'same',\n",
        "                                        activation = tf.nn.relu, name = 'deconv4')# ?*32*32*128\n",
        "    batch4 = tf.layers.batch_normalization(deconv4, training = is_training)\n",
        "    dropout4 = tf.nn.dropout(batch4, dropout_rate)\n",
        "    print(deconv4.shape)\n",
        "    #Output layer\n",
        "    deconv5 = tf.layers.conv2d_transpose(dropout4, 1, kernel_size = [4,4],\n",
        "                                        strides = [2,2], padding = 'same',\n",
        "                                        activation = None, name = 'deconv5')# ?*64*64*1\n",
        "    out = tf.nn.tanh(deconv5)\n",
        "    print(deconv5.shape)\n",
        "    \n",
        "    return out  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uNYagCnMJaLS",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "                ############ Building model ############\n",
        "                \n",
        "def build_GAN(x_real, z, dropout_rate, is_training):\n",
        "    \n",
        "    fake_images = generator(z, dropout_rate, is_training)\n",
        "    \n",
        "    D_real_logits, D_real_prob = discriminator(x_real, dropout_rate, is_training)\n",
        "    \n",
        "    D_fake_logits, D_fake_prob = discriminator(fake_images, dropout_rate,\n",
        "                                                                is_training, reuse = True)\n",
        "    #Setting reuse=True this time for using variables trained in real batch training \n",
        "    \n",
        "    return D_real_logits, D_real_prob, D_fake_logits, D_fake_prob, fake_images \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iGyS5eI1JeE7",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "             ############ Defining losses ############\n",
        "            \n",
        "# The total loss inculcates  D_L_Unsupervised + D_L_Supervised + G_feature_matching loss            \n",
        "def loss_accuracy(D_real_logits, D_fake_logits):\n",
        "    \n",
        "                    ### Discriminator loss ###\n",
        "    \n",
        "    # Unsupervised loss -> R/F\n",
        "    \n",
        "    D_L_Real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
        "            logits = D_real_logits, labels = tf.ones_like(D_real_logits, dtype=tf.float32)))\n",
        "    \n",
        "    D_L_Fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
        "            logits = D_fake_logits, labels = tf.zeros_like(D_fake_logits, dtype=tf.float32)))\n",
        "    \n",
        "    D_L = D_L_Real + D_L_Fake\n",
        "    \n",
        "                    ### Generator loss ###\n",
        "                    \n",
        "    # Fake data wanna be real   \n",
        "    \n",
        "    G_L = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
        "            logits = D_fake_logits, labels = tf.ones_like(D_fake_logits, dtype=tf.float32)))\n",
        "  \n",
        "    return D_L, G_L"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-v8g1_2ZJhkF",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "            ############ Defining Optimizer ############\n",
        "  \n",
        "def optimizer(D_Loss, G_Loss, learning_rate, beta1 = 0.5):\n",
        "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "    with tf.control_dependencies(update_ops):\n",
        "        all_vars = tf.trainable_variables()\n",
        "        D_vars = [var for var in all_vars if var.name.startswith('Discriminator')]\n",
        "        G_vars = [var for var in all_vars if var.name.startswith('Generator')]\n",
        "        \n",
        "        d_train_opt = tf.train.AdamOptimizer(learning_rate, beta1,\n",
        "                                             name = 'd_optimiser').minimize(D_Loss, var_list=D_vars)\n",
        "        g_train_opt = tf.train.AdamOptimizer(learning_rate, beta1,\n",
        "                                             name = 'g_optimiser').minimize(G_Loss, var_list=G_vars)\n",
        "        print ('Done optimising for now ')\n",
        "            \n",
        "    return d_train_opt, g_train_opt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WbrCsvtrM4VF",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "            ############ Plotting Results ############\n",
        "  \n",
        "def show_result(test_images, num_epoch, show = True, save = False, path = 'result.png'):\n",
        "\n",
        "    size_figure_grid = 5\n",
        "    fig, ax = plt.subplots(size_figure_grid, size_figure_grid, figsize=(5, 5))\n",
        "    for i in range(0, size_figure_grid):\n",
        "      for j in range(0, size_figure_grid):\n",
        "        ax[i, j].get_xaxis().set_visible(False)\n",
        "        ax[i, j].get_yaxis().set_visible(False)\n",
        "\n",
        "    for k in range(size_figure_grid*size_figure_grid):\n",
        "        i = k // size_figure_grid\n",
        "        j = k % size_figure_grid\n",
        "        ax[i, j].cla()\n",
        "        ax[i, j].imshow(np.reshape(test_images[k], (64, 64)), cmap='gray')\n",
        "\n",
        "    label = 'Epoch {0}'.format(num_epoch)\n",
        "    fig.text(0.5, 0.04, label, ha='center')\n",
        "\n",
        "    if save:\n",
        "        plt.savefig(path)\n",
        "\n",
        "    if show:\n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.close()\n",
        "\n",
        "def show_train_hist(hist, show = False, save = False, path = 'Train_hist.png'):\n",
        "    \n",
        "    \n",
        "    x = range(len(hist['D_losses']))\n",
        "\n",
        "    y1 = hist['D_losses']\n",
        "    y2 = hist['G_losses']\n",
        "\n",
        "    plt.plot(x, y1, label='D_loss')\n",
        "    plt.plot(x, y2, label='G_loss')\n",
        "\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "\n",
        "    plt.legend(loc=4)\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save:\n",
        "        plt.savefig(path)\n",
        "\n",
        "    if show:\n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.close()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ELqpD5tSJl22",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "            ############ TRAINING ############\n",
        "            \n",
        "def train_GAN(batch_size, epochs):\n",
        "  \n",
        "    train_hist = {}\n",
        "    train_hist['D_losses'] = []\n",
        "    train_hist['G_losses'] = []\n",
        "    train_D_losses, train_G_losses = [], []\n",
        "    \n",
        "    tf.reset_default_graph()\n",
        "    \n",
        "    x = tf.placeholder(tf.float32, shape = [None, height, width, channels], name = 'x')\n",
        "    z = tf.placeholder(tf.float32, shape = [None, 1, 1, latent], name = 'z')\n",
        "    dropout_rate = tf.placeholder(tf.float32, name = 'dropout_rate')\n",
        "    is_training = tf.placeholder(tf.bool, name = 'is_training')\n",
        "    \n",
        "    lr_rate = 2e-4\n",
        "    \n",
        "    model = build_GAN(x, z, dropout_rate, is_training)\n",
        "    D_real_logits, D_real_prob, D_fake_logits, D_fake_prob, fake_data = model\n",
        "    \n",
        "    D_L, G_L = loss_accuracy(D_real_logits, D_fake_logits)\n",
        "\n",
        "    D_optimizer, G_optimizer = optimizer(D_L, G_L, lr_rate, beta1 = 0.5)\n",
        "    \n",
        "    print ('... Training begins ...')\n",
        "    \n",
        "    with tf.Session() as sess:\n",
        "      sess.run(tf.global_variables_initializer())\n",
        "      \n",
        "      mnist_data = get_data()\n",
        "      no_of_batches = int (mnist_data.train.images.shape[0]/batch_size) + 1\n",
        "      \n",
        "      for epoch in range(epochs):\n",
        "        \n",
        "        for it in range(no_of_batches):\n",
        "          \n",
        "          batch = mnist_data.train.next_batch(batch_size, shuffle = False)\n",
        "          # batch[0] has shape: batch_size*28*28*1\n",
        "          #print('Printing shape before resizing: ' + str(batch[0].shape))\n",
        "          batch_reshaped = tf.image.resize_images(batch[0], [64, 64]).eval()\n",
        "          #print('Printing shape after resizing: ' + str(batch_reshaped.shape))\n",
        "          # Reshaping the whole batch into batch_size*64*64*1 for disc/gen architecture\n",
        "          batch_z = np.random.normal(0, 1, (batch_size, 1, 1, latent))\n",
        "                \n",
        "          train_feed_dict = {x : scale(batch_reshaped), z : batch_z, dropout_rate : 0.7,\n",
        "                                   is_training : True}\n",
        "                \n",
        "          D_optimizer.run(feed_dict = train_feed_dict)\n",
        "          G_optimizer.run(feed_dict = train_feed_dict)\n",
        "                \n",
        "          train_D_loss = D_L.eval(feed_dict = train_feed_dict)\n",
        "          train_G_loss = G_L.eval(feed_dict = train_feed_dict)\n",
        "          \n",
        "          train_D_losses.append(train_D_loss)\n",
        "          train_G_losses.append(train_G_loss)\n",
        "          print('Batch evaluated: ' +str(it+1))\n",
        "                \n",
        "        print ('After epoch: '+ str(epoch+1) + ' Generator loss: ' + str(train_G_loss) + ' Discriminator loss: ' + str(train_D_loss) )\n",
        "        \n",
        "        gen_samples = fake_data.eval(feed_dict = {z : np.random.normal(0, 1, (25, 1, 1, latent)), dropout_rate : 0.7, is_training : False})\n",
        "        # Dont train batch-norm while plotting => is_training = False\n",
        "        test_images = tf.image.resize_images(gen_samples, [64, 64]).eval()\n",
        "        show_result(test_images, (epoch + 1), show = True, save = False, path = '')\n",
        "        \n",
        "        train_hist['D_losses'].append(np.mean(train_D_losses))\n",
        "        train_hist['G_losses'].append(np.mean(train_G_losses))\n",
        "        \n",
        "      show_train_hist(train_hist, show=True, save = True, path = 'train_hist.png')\n",
        "      sess.close()\n",
        "            \n",
        "    return train_D_losses,train_G_losses \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cPWy294LRzFr",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "key = train_GAN( 128 , 20)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}